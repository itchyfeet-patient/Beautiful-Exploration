{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CaF7gIrDT-Je2GU4BFxkFFtEZmnUCUpR",
      "authorship_tag": "ABX9TyNLhQcIKKB6vNIu+PX/lrQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itchyfeet-patient/Beautiful-Exploration/blob/master/Exploration_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í”„ë¡œì íŠ¸: ë©‹ì§„ ì‘ì‚¬ê°€ ë§Œë“¤ê¸° ğŸ¹"
      ],
      "metadata": {
        "id": "R6bcExnGERc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ì„ í™•ì¸í•´ ë´…ë‹ˆë‹¤"
      ],
      "metadata": {
        "id": "DtRAXadcF_Qb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkxjuK3cEOxs",
        "outputId": "554acbbc-5e1a-44b7-bf73-77ffd73b697c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
      ],
      "metadata": {
        "id": "vLokBVhdGCla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "aiffel í´ë¼ìš°ë“œ ì£¼í”¼í„°ì— ìˆëŠ” íŒŒì¼ì„ ë°›ì•„ì™”ìŠµë‹ˆë‹¤!"
      ],
      "metadata": {
        "id": "PuSoCx4MGK_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
        "\n",
        "glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë©´ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ì‘ì—…ì„ í•˜ê¸°ê°€ ì•„ì£¼ ìš©ì´í•´ìš”. glob ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, raw_corpus ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ë„ë¡ í• ê²Œìš”!"
      ],
      "metadata": {
        "id": "EUYCvuDGGQWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/dataset/lyricist/data/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŠµë‹ˆë‹¤.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnVklq1WGT8o",
        "outputId": "e796c197-effc-424c-da4a-b55f843df7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°ì´í„° í¬ê¸°: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. ë°ì´í„° ì •ì œ\n"
      ],
      "metadata": {
        "id": "3Xy8SwqmHQLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•ì„œ ë°°ìš´ í…Œí¬ë‹‰ë“¤ì„ í™œìš©í•´ ë¬¸ì¥ ìƒì„±ì— ì í•©í•œ ëª¨ì–‘ìƒˆë¡œ ë°ì´í„°ë¥¼ ì •ì œí•˜ì„¸ìš”!\n",
        "\n",
        "preprocess_sentence() í•¨ìˆ˜ë¥¼ ë§Œë“  ê²ƒì„ ê¸°ì–µí•˜ì‹œì£ ? ì´ë¥¼ í™œìš©í•´ ë°ì´í„°ë¥¼ ì •ì œí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì¶”ê°€ë¡œ ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë¬¸ì¥ì€ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì´ ê³¼ë„í•œ Paddingì„ ê°–ê²Œ í•˜ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤. ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë…¸ë˜ ê°€ì‚¬ ì‘ì‚¬í•˜ê¸°ì— ì–´ìš¸ë¦¬ì§€ ì•Šì„ ìˆ˜ë„ ìˆê² ì£ .\n",
        "ê·¸ë˜ì„œ ì´ë²ˆì—ëŠ” ë¬¸ì¥ì„ í† í°í™” í–ˆì„ ë•Œ í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì„ í•™ìŠµ ë°ì´í„°ì—ì„œ ì œì™¸í•˜ê¸° ë¥¼ ê¶Œí•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "YG8A3vluHUyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus =[]\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "   # if len(sentence.split(' ')) > 15: continue\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    corpus.append(sentence) # ë‹´ê¸°\n",
        "    \n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3tni_2YIlx1",
        "outputId": "39eb6d38-6710-4c03-ec68-743d25d8150d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> Looking for some education <end>',\n",
              " '<start> Made my way into the night <end>',\n",
              " '<start> All that bullshit conversation <end>',\n",
              " \"<start> Baby, can't you read the signs? I won't bore you with the details, baby <end>\",\n",
              " \"<start> I don't even wanna waste your time <end>\",\n",
              " \"<start> Let's just say that maybe <end>\",\n",
              " '<start> You could help me ease my mind <end>',\n",
              " \"<start> I ain't Mr. Right But if you're looking for fast love <end>\",\n",
              " \"<start> If that's love in your eyes <end>\",\n",
              " \"<start> It's more than enough <end>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬"
      ],
      "metadata": {
        "id": "zPEEXYIYHZ4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì„¸ìš”!\n",
        "\n",
        "tokenize() í•¨ìˆ˜ë¡œ ë°ì´í„°ë¥¼ Tensorë¡œ ë³€í™˜í•œ í›„, sklearn ëª¨ë“ˆì˜ train_test_split() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë‹¨ì–´ì¥ì˜ í¬ê¸°ëŠ” 12,000 ì´ìƒ ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”! ì´ ë°ì´í„°ì˜ 20% ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”!\n",
        "\n"
      ],
      "metadata": {
        "id": "r1HJqY7PHbhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    # 12000ë‹¨ì–´ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” tokenizerë¥¼ ë§Œë“¤ê²ë‹ˆë‹¤\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words= 12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±í•©ë‹ˆë‹¤\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤\n",
        "    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
        "    # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjwCIkSd3gC",
        "outputId": "076b6e76-65b7-4f14-d656-b0fd650ad60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 292  22 ...   0   0   0]\n",
            " [  2 214  10 ...   0   0   0]\n",
            " [  2  21  14 ...   0   0   0]\n",
            " ...\n",
            " [  2  92   4 ...   0   0   0]\n",
            " [  2 116   9 ...   0   0   0]\n",
            " [  2  60   4 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fa9b70f4710>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor[:3, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgBsyF9Cehr7",
        "outputId": "c8002b0a-1156-474e-b49d-3723cda5d1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  292   22   87 6868    3    0    0    0    0]\n",
            " [   2  214   10   79  215    4  127    3    0    0]\n",
            " [   2   21   14 1127 2769    3    0    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvOSW9fiflim",
        "outputId": "ae2f1929-29da-4d32-f76e-806562b3b50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : the\n",
            "5 : i\n",
            "6 : you\n",
            "7 : and\n",
            "8 : to\n",
            "9 : a\n",
            "10 : my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
        "# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
        "src_input = tensor[:, :-1]  \n",
        "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MOmDRsffrx7",
        "outputId": "d7cfee11-f12d-4c55-ec62-f6c4e47da828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  292   22   87 6868    3    0    0    0    0    0    0    0    0]\n",
            "[ 292   22   87 6868    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=2022)"
      ],
      "metadata": {
        "id": "anRNHB-uHd3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 12000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 12001ê°œ\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# ì¤€ë¹„í•œ ë°ì´í„° ì†ŒìŠ¤ë¡œë¶€í„° ë°ì´í„°ì…‹ì„ ë§Œë“­ë‹ˆë‹¤\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EWUv1_dgEDh",
        "outputId": "053d6cca-c137-487a-f3ef-3aa9e7198965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcjoAJATxaF0",
        "outputId": "51ece979-a88f-465d-afca-7969b7697967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°"
      ],
      "metadata": {
        "id": "nHUVI8VGHgZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëª¨ë¸ì˜ Embedding Sizeì™€ Hidden Sizeë¥¼ ì¡°ì ˆí•˜ë©° 10 Epoch ì•ˆì— val_loss ê°’ì„ 2.2 ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•˜ì„¸ìš”!\n",
        "\n",
        "ì˜ ì„¤ê³„í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ë©´, model.fit() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. model.fit() í•¨ìˆ˜ì—ëŠ” ë‹¤ì–‘í•œ ì¸ìë¥¼ ë„£ì–´ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ê°€ì¥ ê¸°ë³¸ì ì¸ ì¸ìë¡œëŠ” ë°ì´í„°ì…‹ê³¼ epochsê°€ ìˆìŠµë‹ˆë‹¤. '5. ì‹¤ìŠµ (2) ì¸ê³µì§€ëŠ¥ í•™ìŠµì‹œí‚¤ê¸°'ì—ì„œì˜ ì˜ˆì‹œì™€ ê°™ì´ ë§ì´ì£ .\n",
        "\n",
        "model.fit(dataset, epochs=30)  \n",
        "í•˜ì§€ë§Œ model.fit() í•¨ìˆ˜ì˜ epochsë¥¼ ì•„ë¬´ë¦¬ í¬ê²Œ ë„£ëŠ”ë‹¤ í•´ë„ val_loss ê°’ì€ 2.2 ì•„ë˜ë¡œ ë–¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ´ ê²½ìš°ëŠ” batch sizeë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒê³¼ ê°™ì´ model.fit() í•¨ìˆ˜ì— ë‹¤ì–‘í•œ ì¸ìë¥¼ ë„£ì–´ì£¼ë©´ í•´ê²°ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit ë¥¼ ì°¸ê³ í•˜ì„¸ìš”!\n",
        "\n",
        "LossëŠ” ì•„ë˜ ì œì‹œëœ Loss í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”!"
      ],
      "metadata": {
        "id": "C7g_nLKkHho8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
        "        # ì‚¬ì „ì˜ ì¸ë±ìŠ¤ ê°’ì„ í•´ë‹¹ ì¸ë±ìŠ¤ ë²ˆì§¸ì˜ ì›Œë“œ ë²¡í„°ë¡œ ë°”ê¿”ì¤Œ\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout = 0.3)\n",
        "        \n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 2048\n",
        "# ì›Œë“œ ë²¡í„°ì˜ ì°¨ì›ìˆ˜, ë‹¨ì–´ê°€ ì¶”ìƒì ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” í¬ê¸°\n",
        "hidden_size = 2048\n",
        "# ëª¨ë¸ì— ì–¼ë§ˆë‚˜ ë§ì€ ì¼ê¾¼ì„ ë‘˜ ê²ƒì¸ê°€?\n",
        "\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "thquJBihiCuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropoutì„ í•´ì£¼ë©´ ì •ê·œí™”ê°€ ëœë‹¤ê³  í•´ì„œ ì²«ë²ˆì§¸ rnnì¸µì— 0.3 ë¹„ìœ¨ë¡œ í•´ì¤¬ìŠµë‹ˆë‹¤.  \n",
        "embedding_sizeì™€ hidden_sizeë¥¼ 2048ë¡œ ë°”ê¿”ë´¤ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "X2igCOmpKaAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# í•œ ë°°ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ì–´ë´…ë‹ˆë‹¤\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZknzitzKo6_y",
        "outputId": "f82cb32d-36de-4f8e-a89b-46860c7c4340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
              "array([[[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [-6.31678035e-04,  9.67872096e-04, -5.30990423e-04, ...,\n",
              "         -7.63465214e-05, -1.37594249e-03,  8.99100385e-04],\n",
              "        [-1.01788272e-03,  1.32904842e-03, -1.09078363e-03, ...,\n",
              "          8.31643352e-04, -1.35210564e-03,  1.19861064e-03],\n",
              "        ...,\n",
              "        [ 2.31017405e-03, -1.30087649e-03, -2.80662160e-03, ...,\n",
              "         -1.12886206e-04,  1.12376455e-03,  1.53059140e-03],\n",
              "        [ 2.85678380e-03, -1.16870564e-03, -2.67065014e-03, ...,\n",
              "         -2.57954176e-04,  9.69151559e-04,  2.14233319e-03],\n",
              "        [ 3.17313615e-03, -1.03911071e-03, -2.26436532e-03, ...,\n",
              "         -3.76478478e-04,  8.21163936e-04,  2.94275302e-03]],\n",
              "\n",
              "       [[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [-2.77981017e-04,  8.82921217e-04, -1.74969377e-04, ...,\n",
              "         -1.63461853e-04, -5.94652782e-04,  3.13720113e-04],\n",
              "        [ 6.54933858e-04,  8.91468837e-04, -2.49574863e-04, ...,\n",
              "         -2.40735317e-04, -9.73908755e-04,  2.87539558e-04],\n",
              "        ...,\n",
              "        [ 1.61983841e-03,  3.29980836e-03,  1.92710932e-03, ...,\n",
              "         -2.10957412e-04, -2.02963478e-03,  1.56041526e-03],\n",
              "        [ 1.83142186e-03,  3.27257323e-03,  2.60902406e-03, ...,\n",
              "         -6.11460346e-05, -1.89525180e-03,  2.24644365e-03],\n",
              "        [ 2.02285126e-03,  3.23562301e-03,  3.20482650e-03, ...,\n",
              "          6.64439285e-05, -1.81419670e-03,  2.79868161e-03]],\n",
              "\n",
              "       [[-9.65303043e-05,  2.64193950e-04, -3.54838267e-04, ...,\n",
              "         -1.01451308e-03, -5.16286760e-04,  5.12964441e-04],\n",
              "        [ 2.28825607e-04, -1.38034331e-04, -2.42475857e-04, ...,\n",
              "         -1.71604450e-03, -7.92712381e-04,  8.33983126e-04],\n",
              "        [ 1.11826486e-03,  3.38052996e-05, -5.11068502e-04, ...,\n",
              "         -2.26763682e-03, -3.43176944e-04,  1.20375981e-03],\n",
              "        ...,\n",
              "        [ 1.78095012e-03, -4.41388547e-04,  1.45954220e-03, ...,\n",
              "          1.69895671e-03,  1.81426469e-03,  2.51409691e-03],\n",
              "        [ 1.37895171e-03, -1.09032801e-04,  1.04333437e-03, ...,\n",
              "          1.78993912e-03,  1.79153401e-03,  2.97455257e-03],\n",
              "        [ 1.40896835e-03,  5.18101733e-04, -2.83930131e-05, ...,\n",
              "          1.25395868e-03,  1.48559199e-03,  2.68360972e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-7.71419582e-05,  3.71114693e-05, -4.83428244e-04, ...,\n",
              "          2.85315706e-04, -6.60414051e-04,  1.99094866e-05],\n",
              "        [ 1.35800175e-04,  4.80360381e-04, -5.44726965e-04, ...,\n",
              "          5.18441491e-04, -1.10169826e-03, -1.00331643e-04],\n",
              "        [ 6.48571935e-04,  8.57232430e-04, -7.59372488e-04, ...,\n",
              "          6.46894041e-05, -1.20022439e-03, -1.20236084e-03],\n",
              "        ...,\n",
              "        [-2.11295020e-03, -2.20998586e-03,  9.26359848e-04, ...,\n",
              "          3.31149669e-04, -9.91854933e-04,  4.32787900e-04],\n",
              "        [-2.46023969e-03, -2.27562664e-03,  1.32879568e-03, ...,\n",
              "          2.97156395e-04, -1.35196897e-03,  1.95384448e-04],\n",
              "        [-2.37925816e-03, -2.36289389e-03,  1.62161991e-03, ...,\n",
              "          7.61748292e-04, -1.20625482e-03,  2.66395422e-04]],\n",
              "\n",
              "       [[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [-2.46818847e-04,  8.04920506e-04, -3.80137673e-04, ...,\n",
              "         -5.59834880e-04, -8.91278265e-04,  5.60342916e-04],\n",
              "        [ 3.30861192e-04,  9.20485065e-04, -6.20187377e-04, ...,\n",
              "         -4.95952088e-04, -9.76844458e-04,  5.88751864e-04],\n",
              "        ...,\n",
              "        [ 2.25912523e-03,  2.02551251e-03,  9.28015739e-04, ...,\n",
              "          9.35748161e-04, -1.60821725e-03,  4.96470183e-03],\n",
              "        [ 2.43114633e-03,  2.16029352e-03,  1.67442812e-03, ...,\n",
              "          1.01146672e-03, -1.66167913e-03,  5.09753777e-03],\n",
              "        [ 2.58599245e-03,  2.27735611e-03,  2.33627670e-03, ...,\n",
              "          1.04671693e-03, -1.72172184e-03,  5.12776989e-03]],\n",
              "\n",
              "       [[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [ 7.83779324e-05,  1.01165881e-03, -5.80465887e-04, ...,\n",
              "          4.07407439e-04, -2.99510837e-04,  1.13906234e-03],\n",
              "        [ 6.34520198e-04,  1.41013006e-03, -1.12799776e-03, ...,\n",
              "          8.23455339e-04,  2.36955078e-04,  1.76261191e-03],\n",
              "        ...,\n",
              "        [ 3.03825596e-03,  2.78650643e-03, -3.00393649e-03, ...,\n",
              "         -2.85150949e-04, -2.75470875e-03,  2.27504410e-03],\n",
              "        [ 3.26104881e-03,  2.88860500e-03, -2.18696264e-03, ...,\n",
              "         -1.64200392e-04, -2.78106728e-03,  3.10237752e-03],\n",
              "        [ 3.39284190e-03,  2.92287348e-03, -1.29039842e-03, ...,\n",
              "         -5.39680914e-05, -2.73552397e-03,  3.80205503e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIRpEUUqo-Ew",
        "outputId": "a9fbc884-2aee-49c5-e7f5-a33566c927aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  24578048  \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  33562624  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  33562624  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  24590049  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 116,293,345\n",
            "Trainable params: 116,293,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.fit(dataset, epochs=10, validation_data = val_dataset, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXa1m5yIHlAG",
        "outputId": "63a38329-03dc-4dc9-dbe5-7e01cd04172a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "584/584 [==============================] - 198s 331ms/step - loss: 3.1398 - accuracy: 0.5289 - val_loss: 2.7618 - val_accuracy: 0.5549\n",
            "Epoch 2/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 2.5731 - accuracy: 0.5686 - val_loss: 2.4836 - val_accuracy: 0.5812\n",
            "Epoch 3/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 2.2127 - accuracy: 0.6035 - val_loss: 2.3107 - val_accuracy: 0.6050\n",
            "Epoch 4/10\n",
            "584/584 [==============================] - 193s 330ms/step - loss: 1.8784 - accuracy: 0.6447 - val_loss: 2.1931 - val_accuracy: 0.6280\n",
            "Epoch 5/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 1.5785 - accuracy: 0.6921 - val_loss: 2.1196 - val_accuracy: 0.6479\n",
            "Epoch 6/10\n",
            "584/584 [==============================] - 192s 329ms/step - loss: 1.3323 - accuracy: 0.7371 - val_loss: 2.0886 - val_accuracy: 0.6633\n",
            "Epoch 7/10\n",
            "584/584 [==============================] - 193s 330ms/step - loss: 1.1460 - accuracy: 0.7754 - val_loss: 2.0882 - val_accuracy: 0.6737\n",
            "Epoch 8/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 1.0230 - accuracy: 0.8026 - val_loss: 2.1130 - val_accuracy: 0.6783\n",
            "Epoch 9/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 0.9543 - accuracy: 0.8173 - val_loss: 2.1335 - val_accuracy: 0.6805\n",
            "Epoch 10/10\n",
            "584/584 [==============================] - 192s 329ms/step - loss: 0.9197 - accuracy: 0.8242 - val_loss: 2.1604 - val_accuracy: 0.6813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa9b582d690>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "val_loss ê°€ 2.1604ë¡œ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤?`"
      ],
      "metadata": {
        "id": "KjbDQJMjMTPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # ë‹¨ì–´ í•˜ë‚˜ì”© ì˜ˆì¸¡í•´ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤\n",
        "    #    1. ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤\n",
        "    #    2. ì˜ˆì¸¡ëœ ê°’ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ word indexë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤\n",
        "    #    3. 2ì—ì„œ ì˜ˆì¸¡ëœ word indexë¥¼ ë¬¸ì¥ ë’¤ì— ë¶™ì…ë‹ˆë‹¤\n",
        "    #    4. ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í–ˆë‹¤ë©´ ë¬¸ì¥ ìƒì„±ì„ ë§ˆì¹©ë‹ˆë‹¤\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤ \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "gdquEoY3pVXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "id": "bBqkCRw_HnaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecc9ee73-cbf3-4466-d0d8-9aa83ee60600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> life\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3L3JOexxZYQK",
        "outputId": "a4c6d74e-9bb6-4971-9c5b-aa47734e62ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> life is worth living again <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ íšŒê³ "
      ],
      "metadata": {
        "id": "ycHxb08VHuPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ì²˜ìŒì— ìˆ˜ë™ì ìœ¼ë¡œ ë”°ë¼í•˜ë‹¤ ë³´ë‹ˆ validation dataë¥¼ split í•´ë†“ê³ ë„ í•™ìŠµì•ˆì‹œí‚´. ì´ê±° ë¬¸ì œìˆë‹¤.... ëŠ¥ë™ì ìœ¼ë¡œ ì‚´ì ì •ì‹ ì²´ë¦¬ ğŸ’.\n",
        "2. **embedding sizeì™€ hidden sizeë¥¼** ë†’ì˜€ë”ë‹ˆ val_lossê°€ ê¸‰ê²©íˆ ë‚®ì•„ì¡Œë‹¤. ì²˜ìŒ ìƒê°ìœ¼ë¡œëŠ” **hidden size**ê°€ ì¼ê¾¼ë“¤ì˜ ê°œìˆ˜ë¼ê³  í•´ì„œ, ë§ìœ¼ë©´ ë°°ê°€ ì‚°ìœ¼ë¡œ ê°„ë‹¤ê¸°ì— ì¢€ ì ê²Œ ì¡ì•˜ë”ë‹ˆ ë§Œì¡±í•  ë§Œí•œ ìˆ˜ì¹˜ê°€ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤. ì˜¤íˆë ¤ ì§€ê¸ˆ ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì„œ 2048ì´ ì ì€ ìˆ˜ì¹˜ê°€ ì•„ë‹ˆì—ˆë˜ ê²ƒì¼ê¹Œ? ì˜ë¬¸ì˜ ì—°ì†ì´ë‹¤.\n",
        "3. ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì¡Œë‹¤ ë³´ë‹ˆ ì¢€ ê³¼ì í•©? ë ìˆ˜ë„ ìˆê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì–´ì„œ ì–´ì œ ì¹´í›—ì— ì ê¹ ë‚˜ì˜¨ **dropoutì„ ì‚¬ìš©í•´ë´¤ë‹¤**. RNN êµ¬ê¸€ë§ í•˜ë‹¤ ë³´ë‹ˆ RNN ì¸µì— ì˜µì…˜ìœ¼ë¡œ dropout ë¹„ìœ¨ì„ ì§€ì •í•´ì£¼ëŠ” ê²½ìš°ê°€ ìˆì–´ì„œ. **0.3**ìœ¼ë¡œ ì§€ì •í•´ ì¤¬ë”ë‹ˆ ì¡°ê¸ˆ ? ë¹¨ë¼ì§„ ê²ƒ ê°™ê¸°ë„ í•˜ê³  val_lossê°€ ê°ì†Œëœ ê²ƒ ê°™ê¸°ë„ í•˜ê³ ..   \n",
        "    ê·¸ëŸ°ë° ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ ìœ„í•´ì„œëŠ” dropoutì„ ì§€ì–‘í•œë‹¤ê³  í•œë‹¤. ì‚¬ì‹¤ ê·¸ë ‡ì§€. ê°€ì§€ì¹˜ê¸°ë¥¼ í•˜ëŠ”ë° ì™„ì „ ì •í™•í•˜ì§„ ì•Šê² ì§€?\n",
        "4. **í•™ìŠµì‹œê°„ì´ ê¸¸ë‹¤**ë³´ë‹ˆ ë‹¤ì–‘í•œ ì‹œë„ë¥¼ í•´ ë³´ê¸°ë„ ì–´ë ¤ì› ê³  ë§¤ë²ˆ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì³ì„œ ì´ ê²°ê³¼ê°€ ë‚˜ì˜¨ê±´ì§€ ëª¨í˜¸í–ˆë‹¤. (5ë²ˆì—ì„œ ëŠë‚€ ê²ƒ ì²˜ëŸ¼)  \n",
        "    ê·¸ë˜ì„œ ê¼¼ìˆ˜ë¡œ epochë¥¼ ì¡°ê¸ˆ ëŒë ¤ë´ì„œ ê°€ë§ìˆëŠ”(?) val_loss ìˆ˜ì¹˜ê°€ ë‚˜ì˜¤ë©´ epochë¥¼ ëŠ˜ë ¤ì„œ ê²°ê³¼ë¥¼ ë½‘ì•˜ë‹¤. í¬í¬\n",
        "5. tensorflowê°€ ì°¸ ê°„í¸í•˜ë‹¤. tensorflowì˜ í•¨ìˆ˜ë¡œ tokenizeë„ í•˜ê³  ì‚¬ì „ ì¸ë±ìŠ¤ê°’ì„ ì›Œë“œ ë²¡í„°ë¡œë„ ë°”ê¿”ì£¼ê³ .. ìµì˜¤\n",
        "6. ì²˜ìŒì— `if len(sentence.split(' ')) > 15: continue` ë¡œ í† í°ì˜ ê°œìˆ˜ë¥¼ ì¡°ì ˆí–ˆëŠ”ë°, **tensorì˜ êµ¬ì¡°ê°€ 15ë¥¼ ë„˜ëŠ”ê²ƒ**ì— ë­”ê°€ ì´ìƒí•˜ê³  ëŠê¼ˆê³  í† í°ì˜ ê°œìˆ˜ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ë¯€ë¡œ tokenize í•  ë•Œ maxlenì„ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ë§ë‹¤ê³  ìƒê°í•´ì„œ ìˆ˜ì •í–ˆë‹¤.\n",
        "7. ë¬¸ì¥ì„ ë§Œë“  ê±¸ ë³´ë‹ˆ ë‹¤ ê¸°ì¡´ì— ìˆë˜ ê°€ì‚¬ì˜ ë¬¸ì¥ì´ í†µìœ¼ë¡œ ìì£¼ ë‚˜ì™”ëŠ”ë° ì´ëŸ¬ë©´ **í‘œì ˆì‹œë¹„**ì— íœ˜ë§ë¦¬ì§€ ì•Šì„ê¹Œ? í•˜ëŠ” ìƒê°ì´ ë“¬  \n",
        "    ê·¸ë˜ì„œ ë‹¨ì–´ì˜ tokenizeê°€ ì˜ ì•ˆëœê²Œ ì•„ë‹Œì§€ í™•ì¸í•´ë´¤ëŠ”ë° ê·¸ê±´ ìƒê´€ì—†ì—ˆê³ ...  \n",
        "    ì˜ˆì¸¡ëœ ê°’ ì¤‘ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í™•ë¥ ì´ ê³µêµë¡­ê²Œë„ ê·¸ê²ƒì´ì—ˆë˜ ê²ƒì´ì—ˆë˜ ê²ƒì´ì—ˆì„ê¹Œ..? \n",
        "8. í•™ìŠµí•œ ëª¨ë¸ë¡œ ì²˜ìŒ ì‘ì‚¬ ì‹œì¼°ì„ ë•Œ ì•„ëŠ” ë…¸ë˜ê°€ ë‚˜ì™”ë‹¤... Eminemì˜ Love the way you lie... ì—„ì²­ ì¸ê¸°ìˆì—ˆë˜ ë¼ë–¼ë…¸ëœë° ìš”ì¦˜ì• ë“¤ ì•Œê¹Œ? ë‹ˆ ë•ë¶„ì— ì˜¤ëœë§Œì— ë“¤ì—ˆì–´ ê³ ë§ˆì›Œ ì¸ê³µì§€ëŠ¥ì¹œêµ¬ì•¼! ğŸ¤–\n",
        "9. ì•„ë¬´ë¦¬ ì‚¬ë‘íƒ€ë ¹í•˜ëŠ” ë…¸ë˜ê°€ ë§ë‹¤ì§€ë§Œ i love ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì€ ì‹ìƒí•˜ë‹¤ê³  ìƒê°í•´ì„œ lifeë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì–´ë³´ë¼ê³  ì‹œì¼°ë”ë‹ˆ **'life is worth living again'** ë¼ëŠ” ì €ìŠ¤í‹´ë¹„ë²„ì˜ ë…¸ë˜ ì œëª©ì´ ë‚˜ì™”ë‹¤. ì¸ìƒì€ ë‹¤ì‹œ ì‚´ì•„ë³¼ë§Œ í•˜ì§€!"
      ],
      "metadata": {
        "id": "97w-zD9XMhSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¯ í‰ê°€ ë£¨ë¸Œë¦­\n",
        "| **í‰ê°€ë¬¸í•­** | **ìƒì„¸ê¸°ì¤€** | **í•™ìŠµê²°ê³¼** |\n",
        "|---|---|:---:|\n",
        "| 1. ê°€ì‚¬ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ê°€? | í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´ì…˜ ê²°ê³¼ê°€ ê·¸ëŸ´ë“¯í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±ë˜ëŠ”ê°€? | O |\n",
        "| 2. ë°ì´í„°ì˜ ì „ì²˜ë¦¬ì™€ ë°ì´í„°ì…‹ êµ¬ì„± ê³¼ì •ì´ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ê°€? | íŠ¹ìˆ˜ë¬¸ì ì œê±°, í† í¬ë‚˜ì´ì € ìƒì„±, íŒ¨ë”©ì²˜ë¦¬ ë“±ì˜ ê³¼ì •ì´ ë¹ ì§ì—†ì´ ì§„í–‰ë˜ì—ˆëŠ”ê°€? | O |\n",
        "| 3. í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆëŠ”ê°€? | í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì˜ validation lossê°€ 2.2 ì´í•˜ë¡œ ë‚®ì•„ì¡ŒëŠ”ê°€? | Validation loss : 2.1604 |"
      ],
      "metadata": {
        "id": "7mXx3iu1Hv30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì°¸ê³ ìë£Œ\n",
        "Dropout : https://deepestdocs.readthedocs.io/en/latest/004_deep_learning_part_2/0042/  \n",
        "RNN : https://davinci-ai.tistory.com/30"
      ],
      "metadata": {
        "id": "PJPQVCexGjKX"
      }
    }
  ]
}