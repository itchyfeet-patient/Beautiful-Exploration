{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CaF7gIrDT-Je2GU4BFxkFFtEZmnUCUpR",
      "authorship_tag": "ABX9TyNLhQcIKKB6vNIu+PX/lrQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itchyfeet-patient/Beautiful-Exploration/blob/master/Exploration_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트: 멋진 작사가 만들기 🎹"
      ],
      "metadata": {
        "id": "R6bcExnGERc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 버전을 확인해 봅니다"
      ],
      "metadata": {
        "id": "DtRAXadcF_Qb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkxjuK3cEOxs",
        "outputId": "554acbbc-5e1a-44b7-bf73-77ffd73b697c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. 데이터 다운로드"
      ],
      "metadata": {
        "id": "vLokBVhdGCla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "aiffel 클라우드 주피터에 있는 파일을 받아왔습니다!"
      ],
      "metadata": {
        "id": "PuSoCx4MGK_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. 데이터 읽어오기\n",
        "\n",
        "glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요. glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!"
      ],
      "metadata": {
        "id": "EUYCvuDGGQWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/dataset/lyricist/data/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnVklq1WGT8o",
        "outputId": "e796c197-effc-424c-da4a-b55f843df7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. 데이터 정제\n"
      ],
      "metadata": {
        "id": "3Xy8SwqmHQLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
        "\n",
        "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
        "\n",
        "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n",
        "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다."
      ],
      "metadata": {
        "id": "YG8A3vluHUyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus =[]\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "   # if len(sentence.split(' ')) > 15: continue\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    corpus.append(sentence) # 담기\n",
        "    \n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3tni_2YIlx1",
        "outputId": "39eb6d38-6710-4c03-ec68-743d25d8150d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> Looking for some education <end>',\n",
              " '<start> Made my way into the night <end>',\n",
              " '<start> All that bullshit conversation <end>',\n",
              " \"<start> Baby, can't you read the signs? I won't bore you with the details, baby <end>\",\n",
              " \"<start> I don't even wanna waste your time <end>\",\n",
              " \"<start> Let's just say that maybe <end>\",\n",
              " '<start> You could help me ease my mind <end>',\n",
              " \"<start> I ain't Mr. Right But if you're looking for fast love <end>\",\n",
              " \"<start> If that's love in your eyes <end>\",\n",
              " \"<start> It's more than enough <end>\"]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4. 평가 데이터셋 분리"
      ],
      "metadata": {
        "id": "zPEEXYIYHZ4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터와 평가 데이터를 분리하세요!\n",
        "\n",
        "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상 으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!\n",
        "\n"
      ],
      "metadata": {
        "id": "r1HJqY7PHbhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    # 12000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words= 12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zjwCIkSd3gC",
        "outputId": "076b6e76-65b7-4f14-d656-b0fd650ad60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 292  22 ...   0   0   0]\n",
            " [  2 214  10 ...   0   0   0]\n",
            " [  2  21  14 ...   0   0   0]\n",
            " ...\n",
            " [  2  92   4 ...   0   0   0]\n",
            " [  2 116   9 ...   0   0   0]\n",
            " [  2  60   4 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fa9b70f4710>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor[:3, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgBsyF9Cehr7",
        "outputId": "c8002b0a-1156-474e-b49d-3723cda5d1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2  292   22   87 6868    3    0    0    0    0]\n",
            " [   2  214   10   79  215    4  127    3    0    0]\n",
            " [   2   21   14 1127 2769    3    0    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvOSW9fiflim",
        "outputId": "ae2f1929-29da-4d32-f76e-806562b3b50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : the\n",
            "5 : i\n",
            "6 : you\n",
            "7 : and\n",
            "8 : to\n",
            "9 : a\n",
            "10 : my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]  \n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MOmDRsffrx7",
        "outputId": "d7cfee11-f12d-4c55-ec62-f6c4e47da828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  292   22   87 6868    3    0    0    0    0    0    0    0    0]\n",
            "[ 292   22   87 6868    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=2022)"
      ],
      "metadata": {
        "id": "anRNHB-uHd3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EWUv1_dgEDh",
        "outputId": "053d6cca-c137-487a-f3ef-3aa9e7198965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcjoAJATxaF0",
        "outputId": "51ece979-a88f-465d-afca-7969b7697967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. 인공지능 만들기"
      ],
      "metadata": {
        "id": "nHUVI8VGHgZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\n",
        "\n",
        "잘 설계한 모델을 학습하려면, model.fit() 함수를 사용해야 합니다. model.fit() 함수에는 다양한 인자를 넣어주어야 하는데, 가장 기본적인 인자로는 데이터셋과 epochs가 있습니다. '5. 실습 (2) 인공지능 학습시키기'에서의 예시와 같이 말이죠.\n",
        "\n",
        "model.fit(dataset, epochs=30)  \n",
        "하지만 model.fit() 함수의 epochs를 아무리 크게 넣는다 해도 val_loss 값은 2.2 아래로 떨어지지 않습니다. 이럴 경우는 batch size를 변경하는 것과 같이 model.fit() 함수에 다양한 인자를 넣어주면 해결될 수도 있습니다. 자세한 내용은 https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit 를 참고하세요!\n",
        "\n",
        "Loss는 아래 제시된 Loss 함수를 그대로 사용하세요!"
      ],
      "metadata": {
        "id": "C7g_nLKkHho8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
        "        # 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔줌\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True, dropout = 0.3)\n",
        "        \n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 2048\n",
        "# 워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기\n",
        "hidden_size = 2048\n",
        "# 모델에 얼마나 많은 일꾼을 둘 것인가?\n",
        "\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "thquJBihiCuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dropout을 해주면 정규화가 된다고 해서 첫번째 rnn층에 0.3 비율로 해줬습니다.  \n",
        "embedding_size와 hidden_size를 2048로 바꿔봤습니다."
      ],
      "metadata": {
        "id": "X2igCOmpKaAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZknzitzKo6_y",
        "outputId": "f82cb32d-36de-4f8e-a89b-46860c7c4340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
              "array([[[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [-6.31678035e-04,  9.67872096e-04, -5.30990423e-04, ...,\n",
              "         -7.63465214e-05, -1.37594249e-03,  8.99100385e-04],\n",
              "        [-1.01788272e-03,  1.32904842e-03, -1.09078363e-03, ...,\n",
              "          8.31643352e-04, -1.35210564e-03,  1.19861064e-03],\n",
              "        ...,\n",
              "        [ 2.31017405e-03, -1.30087649e-03, -2.80662160e-03, ...,\n",
              "         -1.12886206e-04,  1.12376455e-03,  1.53059140e-03],\n",
              "        [ 2.85678380e-03, -1.16870564e-03, -2.67065014e-03, ...,\n",
              "         -2.57954176e-04,  9.69151559e-04,  2.14233319e-03],\n",
              "        [ 3.17313615e-03, -1.03911071e-03, -2.26436532e-03, ...,\n",
              "         -3.76478478e-04,  8.21163936e-04,  2.94275302e-03]],\n",
              "\n",
              "       [[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [-2.77981017e-04,  8.82921217e-04, -1.74969377e-04, ...,\n",
              "         -1.63461853e-04, -5.94652782e-04,  3.13720113e-04],\n",
              "        [ 6.54933858e-04,  8.91468837e-04, -2.49574863e-04, ...,\n",
              "         -2.40735317e-04, -9.73908755e-04,  2.87539558e-04],\n",
              "        ...,\n",
              "        [ 1.61983841e-03,  3.29980836e-03,  1.92710932e-03, ...,\n",
              "         -2.10957412e-04, -2.02963478e-03,  1.56041526e-03],\n",
              "        [ 1.83142186e-03,  3.27257323e-03,  2.60902406e-03, ...,\n",
              "         -6.11460346e-05, -1.89525180e-03,  2.24644365e-03],\n",
              "        [ 2.02285126e-03,  3.23562301e-03,  3.20482650e-03, ...,\n",
              "          6.64439285e-05, -1.81419670e-03,  2.79868161e-03]],\n",
              "\n",
              "       [[-9.65303043e-05,  2.64193950e-04, -3.54838267e-04, ...,\n",
              "         -1.01451308e-03, -5.16286760e-04,  5.12964441e-04],\n",
              "        [ 2.28825607e-04, -1.38034331e-04, -2.42475857e-04, ...,\n",
              "         -1.71604450e-03, -7.92712381e-04,  8.33983126e-04],\n",
              "        [ 1.11826486e-03,  3.38052996e-05, -5.11068502e-04, ...,\n",
              "         -2.26763682e-03, -3.43176944e-04,  1.20375981e-03],\n",
              "        ...,\n",
              "        [ 1.78095012e-03, -4.41388547e-04,  1.45954220e-03, ...,\n",
              "          1.69895671e-03,  1.81426469e-03,  2.51409691e-03],\n",
              "        [ 1.37895171e-03, -1.09032801e-04,  1.04333437e-03, ...,\n",
              "          1.78993912e-03,  1.79153401e-03,  2.97455257e-03],\n",
              "        [ 1.40896835e-03,  5.18101733e-04, -2.83930131e-05, ...,\n",
              "          1.25395868e-03,  1.48559199e-03,  2.68360972e-03]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-7.71419582e-05,  3.71114693e-05, -4.83428244e-04, ...,\n",
              "          2.85315706e-04, -6.60414051e-04,  1.99094866e-05],\n",
              "        [ 1.35800175e-04,  4.80360381e-04, -5.44726965e-04, ...,\n",
              "          5.18441491e-04, -1.10169826e-03, -1.00331643e-04],\n",
              "        [ 6.48571935e-04,  8.57232430e-04, -7.59372488e-04, ...,\n",
              "          6.46894041e-05, -1.20022439e-03, -1.20236084e-03],\n",
              "        ...,\n",
              "        [-2.11295020e-03, -2.20998586e-03,  9.26359848e-04, ...,\n",
              "          3.31149669e-04, -9.91854933e-04,  4.32787900e-04],\n",
              "        [-2.46023969e-03, -2.27562664e-03,  1.32879568e-03, ...,\n",
              "          2.97156395e-04, -1.35196897e-03,  1.95384448e-04],\n",
              "        [-2.37925816e-03, -2.36289389e-03,  1.62161991e-03, ...,\n",
              "          7.61748292e-04, -1.20625482e-03,  2.66395422e-04]],\n",
              "\n",
              "       [[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [-2.46818847e-04,  8.04920506e-04, -3.80137673e-04, ...,\n",
              "         -5.59834880e-04, -8.91278265e-04,  5.60342916e-04],\n",
              "        [ 3.30861192e-04,  9.20485065e-04, -6.20187377e-04, ...,\n",
              "         -4.95952088e-04, -9.76844458e-04,  5.88751864e-04],\n",
              "        ...,\n",
              "        [ 2.25912523e-03,  2.02551251e-03,  9.28015739e-04, ...,\n",
              "          9.35748161e-04, -1.60821725e-03,  4.96470183e-03],\n",
              "        [ 2.43114633e-03,  2.16029352e-03,  1.67442812e-03, ...,\n",
              "          1.01146672e-03, -1.66167913e-03,  5.09753777e-03],\n",
              "        [ 2.58599245e-03,  2.27735611e-03,  2.33627670e-03, ...,\n",
              "          1.04671693e-03, -1.72172184e-03,  5.12776989e-03]],\n",
              "\n",
              "       [[-4.34777816e-04,  6.19832077e-04, -2.90790540e-05, ...,\n",
              "         -2.02905823e-04, -3.64403852e-04,  6.13549957e-04],\n",
              "        [ 7.83779324e-05,  1.01165881e-03, -5.80465887e-04, ...,\n",
              "          4.07407439e-04, -2.99510837e-04,  1.13906234e-03],\n",
              "        [ 6.34520198e-04,  1.41013006e-03, -1.12799776e-03, ...,\n",
              "          8.23455339e-04,  2.36955078e-04,  1.76261191e-03],\n",
              "        ...,\n",
              "        [ 3.03825596e-03,  2.78650643e-03, -3.00393649e-03, ...,\n",
              "         -2.85150949e-04, -2.75470875e-03,  2.27504410e-03],\n",
              "        [ 3.26104881e-03,  2.88860500e-03, -2.18696264e-03, ...,\n",
              "         -1.64200392e-04, -2.78106728e-03,  3.10237752e-03],\n",
              "        [ 3.39284190e-03,  2.92287348e-03, -1.29039842e-03, ...,\n",
              "         -5.39680914e-05, -2.73552397e-03,  3.80205503e-03]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIRpEUUqo-Ew",
        "outputId": "a9fbc884-2aee-49c5-e7f5-a33566c927aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  24578048  \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  33562624  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  33562624  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  24590049  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 116,293,345\n",
            "Trainable params: 116,293,345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "model.fit(dataset, epochs=10, validation_data = val_dataset, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXa1m5yIHlAG",
        "outputId": "63a38329-03dc-4dc9-dbe5-7e01cd04172a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "584/584 [==============================] - 198s 331ms/step - loss: 3.1398 - accuracy: 0.5289 - val_loss: 2.7618 - val_accuracy: 0.5549\n",
            "Epoch 2/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 2.5731 - accuracy: 0.5686 - val_loss: 2.4836 - val_accuracy: 0.5812\n",
            "Epoch 3/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 2.2127 - accuracy: 0.6035 - val_loss: 2.3107 - val_accuracy: 0.6050\n",
            "Epoch 4/10\n",
            "584/584 [==============================] - 193s 330ms/step - loss: 1.8784 - accuracy: 0.6447 - val_loss: 2.1931 - val_accuracy: 0.6280\n",
            "Epoch 5/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 1.5785 - accuracy: 0.6921 - val_loss: 2.1196 - val_accuracy: 0.6479\n",
            "Epoch 6/10\n",
            "584/584 [==============================] - 192s 329ms/step - loss: 1.3323 - accuracy: 0.7371 - val_loss: 2.0886 - val_accuracy: 0.6633\n",
            "Epoch 7/10\n",
            "584/584 [==============================] - 193s 330ms/step - loss: 1.1460 - accuracy: 0.7754 - val_loss: 2.0882 - val_accuracy: 0.6737\n",
            "Epoch 8/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 1.0230 - accuracy: 0.8026 - val_loss: 2.1130 - val_accuracy: 0.6783\n",
            "Epoch 9/10\n",
            "584/584 [==============================] - 193s 329ms/step - loss: 0.9543 - accuracy: 0.8173 - val_loss: 2.1335 - val_accuracy: 0.6805\n",
            "Epoch 10/10\n",
            "584/584 [==============================] - 192s 329ms/step - loss: 0.9197 - accuracy: 0.8242 - val_loss: 2.1604 - val_accuracy: 0.6813\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa9b582d690>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "val_loss 가 2.1604로 만족스럽습니다?`"
      ],
      "metadata": {
        "id": "KjbDQJMjMTPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "gdquEoY3pVXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "metadata": {
        "id": "bBqkCRw_HnaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecc9ee73-cbf3-4466-d0d8-9aa83ee60600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> life\", max_len=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3L3JOexxZYQK",
        "outputId": "a4c6d74e-9bb6-4971-9c5b-aa47734e62ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> life is worth living again <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✍ 회고"
      ],
      "metadata": {
        "id": "ycHxb08VHuPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 처음에 수동적으로 따라하다 보니 validation data를 split 해놓고도 학습안시킴. 이거 문제있다.... 능동적으로 살자 정신체리 🍒.\n",
        "2. **embedding size와 hidden size를** 높였더니 val_loss가 급격히 낮아졌다. 처음 생각으로는 **hidden size**가 일꾼들의 개수라고 해서, 많으면 배가 산으로 간다기에 좀 적게 잡았더니 만족할 만한 수치가 나오지 않았다. 오히려 지금 데이터 수가 많아서 2048이 적은 수치가 아니었던 것일까? 의문의 연속이다.\n",
        "3. 데이터 수가 많아졌다 보니 좀 과적합? 될수도 있겠다는 생각이 들어서 어제 카훗에 잠깐 나온 **dropout을 사용해봤다**. RNN 구글링 하다 보니 RNN 층에 옵션으로 dropout 비율을 지정해주는 경우가 있어서. **0.3**으로 지정해 줬더니 조금 ? 빨라진 것 같기도 하고 val_loss가 감소된 것 같기도 하고..   \n",
        "    그런데 결과의 정확도를 위해서는 dropout을 지양한다고 한다. 사실 그렇지. 가지치기를 하는데 완전 정확하진 않겠지?\n",
        "4. **학습시간이 길다**보니 다양한 시도를 해 보기도 어려웠고 매번 어떤 파라미터를 고쳐서 이 결과가 나온건지 모호했다. (5번에서 느낀 것 처럼)  \n",
        "    그래서 꼼수로 epoch를 조금 돌려봐서 가망있는(?) val_loss 수치가 나오면 epoch를 늘려서 결과를 뽑았다. 크크\n",
        "5. tensorflow가 참 간편하다. tensorflow의 함수로 tokenize도 하고 사전 인덱스값을 워드 벡터로도 바꿔주고.. 쵝오\n",
        "6. 처음에 `if len(sentence.split(' ')) > 15: continue` 로 토큰의 개수를 조절했는데, **tensor의 구조가 15를 넘는것**에 뭔가 이상하고 느꼈고 토큰의 개수를 조절하는 것이므로 tokenize 할 때 maxlen을 조절하는 것이 맞다고 생각해서 수정했다.\n",
        "7. 문장을 만든 걸 보니 다 기존에 있던 가사의 문장이 통으로 자주 나왔는데 이러면 **표절시비**에 휘말리지 않을까? 하는 생각이 듬  \n",
        "    그래서 단어의 tokenize가 잘 안된게 아닌지 확인해봤는데 그건 상관없었고...  \n",
        "    예측된 값 중 확률이 가장 높은 확률이 공교롭게도 그것이었던 것이었던 것이었을까..? \n",
        "8. 학습한 모델로 처음 작사 시켰을 때 아는 노래가 나왔다... Eminem의 Love the way you lie... 엄청 인기있었던 라떼노랜데 요즘애들 알까? 니 덕분에 오랜만에 들었어 고마워 인공지능친구야! 🤖\n",
        "9. 아무리 사랑타령하는 노래가 많다지만 i love 로 시작하는 문장은 식상하다고 생각해서 life로 시작하는 문장을 만들어보라고 시켰더니 **'life is worth living again'** 라는 저스틴비버의 노래 제목이 나왔다. 인생은 다시 살아볼만 하지!"
      ],
      "metadata": {
        "id": "97w-zD9XMhSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🎯 평가 루브릭\n",
        "| **평가문항** | **상세기준** | **학습결과** |\n",
        "|---|---|:---:|\n",
        "| 1. 가사 텍스트 생성 모델이 정상적으로 동작하는가? | 텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가? | O |\n",
        "| 2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가? | 특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었는가? | O |\n",
        "| 3. 텍스트 생성모델이 안정적으로 학습되었는가? | 텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가? | Validation loss : 2.1604 |"
      ],
      "metadata": {
        "id": "7mXx3iu1Hv30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 참고자료\n",
        "Dropout : https://deepestdocs.readthedocs.io/en/latest/004_deep_learning_part_2/0042/  \n",
        "RNN : https://davinci-ai.tistory.com/30"
      ],
      "metadata": {
        "id": "PJPQVCexGjKX"
      }
    }
  ]
}